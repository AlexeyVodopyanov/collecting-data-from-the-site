# Сбор данных с сайта

Что было сделано: Опишем процесс, от выбора инструментов до организации парсинга.

Откуда были получены данные: Укажем источник и его URL, объясним цель сбора и назначение данных.

Как осуществлялся сбор: Объясним реализацию (например, использование BeautifulSoup, requests и обход страниц через кнопку "Next").

Почему были выбраны данные инструменты: Например, BeautifulSoup и requests подходят для парсинга небольших сайтов, где нет защиты от ботов, а Scrapy был бы избыточен для данной задачи.

## 1. Подготовка к сбору данных:
- Проанализируем HTML-структуру сайта https://quotes.toscrape.com/  для выявления необходимых данных и их расположения в HTML-документе.
- Целевые данные для сбора: цитаты, авторы и теги.

## 2. Сбор данных:
- Используем библиотеку BeautifulSoup для парсинга HTML-страниц, а requests для получения их содержимого.
- Организуем парсинг таким образом, чтобы обойти все страницы, собирая данные с каждой из них.
- Каждая страница имеет кнопку "Next", что позволяет определить, когда парсинг страниц завершен.

## 3. Запись результатов в JSON:
- Собранные данные будут записаны в JSON-файл, структура которого будет такой:
```
[
  {
    "quote": "Текст цитаты",
    "author": "Автор",
    "tags": ["тег1", "тег2"]
  },
  ...
]
```
